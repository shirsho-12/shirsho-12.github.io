---
layout: post
title:  Monte Carlo - Learning from Experience
subtitle: RL Basics
permalink: /blog/rl_mc/
tags: [Reinforcement Learning, Monte Carlo, MC Control, RL Basics]
comments: true
share-img: https://raw.githubusercontent.com/shirsho-12/shirsho-12.github.io/master/images/rl/rl_theme.png
---

Last time we looked at dynamic programming, methods that compute near-exact solutions to problems using a full-scale model of the environment dynamics. This time we are doing a complete 180. Monte Carlo(MC) methods are everything DP is not. 

Monte Carlo methods consist of interacting directly with the environment - **simulated experience**, and then **averaging rewards** through the states the model went through. Monte Carlo methods in general refer to estimation methods where actions are often completely randomized.

- Complete knowledge of the environment is unnecessary. 
- Works based on averaging outcomes (more on that later)
- Easily scalable to larger problems.
- Estimates are independent, i.e. no bootstrapping
- Only a single sequence of events is looked into at each iteration (no full backups)

Don't let this fool you, the methods we have seen in DP are still very important in Monte Carlo methods. The iterative cycle of evaluation followed by improvement (known as the General Policy Iteration or GPI) seen before is used in MC.

When we say that complete environment knowledge is unnecessary or MC methods do not require environment models, we mean that the agent does not have access to the complete environment; it can only interact with the environment and receive rewards.


## Conclusion

A drawback of Monte Carlo methods is that these algorithms only work on episodic tasks. Defining end points for continuing tasks to take averages from is far more challenging.