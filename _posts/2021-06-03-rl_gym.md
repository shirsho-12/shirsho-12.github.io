---
layout: post
title:  The OpenAI Gym
subtitle: RL Basics
permalink: /blog/rl_gym/
tags: [Reinforcement Learning, OpenAI Gym, Games and AI, RL Basics]
comments: true
share-img: https://raw.githubusercontent.com/shirsho-12/shirsho-12.github.io/master/images/rl/rl_theme.png
---

The OpenAI Gym is a fascinating place. The Gym makes playing with reinforcement learning models fun and interactive without having to deal with the hassle of setting up environments. While you could argue that creating your own environments is a pretty important skill, do you really want to spend a week in something like PyGame just to start a project? No? Well let's get right to it then.

According to OpenAI, _Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball._ And it is, to date, the best place to play around with RL. It is easy to use, well-documented, and very customizable. Go to  the [Table of Environments](https://github.com/openai/gym/wiki/Table-of-environments) for a breakdown of all they have to offer. Without further ado, let's get to work!

## Setup

To start working with the OpenAI Gym, we need two libraries:
- [**Anaconda**](www.anaconda.com/): This is an open-source Python distribution for data science and machine learning. Anaconda has its own package manager `conda`, which works alongside `pip`. The installation process is simple and the [Products page](https://www.anaconda.com/products/individual) has easy-to-use installers.
- [**PyTorch**](https://pytorch.org/)<sup>[1](#myfootnote1)</sup>: This is an AI/ML library from the Facebook AI Research(FAIR) group. It is similar to NumPy in matrix manipulation, but is more optimized and allows GPU support. Just go to [the Pytorch website](https://pytorch.org/) and select your system. CUDA is for NVidia GPUs, so if you don't have one or have an AMD GPU (like me), select the CPU option.

<figure>
    <img src="/images/rl/pytorch_install.png" desc="Installing PyTorch">
    <figcaption>Select the relevant options from the website and run the resulting line of code on your system.</figcaption>
</figure>

## Going to the Gym
Setting up the Gym and running environments is a very simple process. 

    pip install gym

That's it.

That was Part 1 of the setup. Next we need to install a few modules. Here I will be showing you the Box2D (Lunar Lander) and Atari (All Atari games) environment setups for now.

For Box2D:

    conda install swig
    pip install box2d
    pip install gym[box2d]

For Atari, there are two ways:

    pip install gym[atari]

If this throws any errors, use this instead:
    
    pip install -f https://github.com/Kojoley/atari-py/releases atari_py 

On Windows systems, you may need to install Microsoft Visual Studio Build Tools ([from here](https://visualstudio.microsoft.com/visual-cpp-build-tools/)).This will be the case if you see any errors involving `\atari_py\ale_interface\ale_c.dll`.


## First Steps

Let's start with something iconic from the Atari 2600, **Ms. Pac-Man**. We shall simulate the game here using the OpenAI Gym. Let's first import the `gym` library.

```py
import gym
```

To create an instance of an environment we use the command `gym.make("env_name")`

```py
env = gym.make("MsPacman-v0")
env.reset()
```

`env.reset()` initializes the environment to a default value.

To see the agent play the game, we render the environment.

```py
env.render()
>>>True
```
<img style="float: right;" src="/images/rl/pacman.png" desc="Ms. Pac-Man simulated">

As you can see, Ms. Pac-Man starts with 3 lives and has an initial score of 0.

We then make our agent Pac-Man perform a random move. 

```py
action = env.action_space.sample()
new_state, reward, is_done, info = env.step(action)
```

Let's break this down. The `action_space` consists of the possible actions that can be done in an environment. `sample()` randomly chooses an action. Here we can see this:

```py
env.action_space.n         # Number of possible actions the agent can take 
>>> 9 
env.get_action_meanings()  # List of actions, NOOP means no actions. 
# All actions correspond to their own numbers eg. 0 -> NOOP, 1-> UP etc.
>>> ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'UPRIGHT', 
>>>  'UPLEFT', 'DOWNRIGHT', 'DOWNLEFT']
env.action_space.sample()  # Randomly selects a number. 2 -> RIGHT
>>> 2
```

`env.step(action)` is our agent taking the specified action. `new_state` refers to the new position of the environment after our agent has taken a step. `reward` is the immediate reward from the action; `is_done` is a boolean variable which signals the termination of the game; and `info` sometimes contains extra information. In this case:

```py 
print(info)               # Number of lives left 
>>> {'ale.lives': 3}
```

If we put all this in a while loop, our agent performs decently.

```py
is_done = False
while not is_done:                        # Loop until game ends
    action = env.action_space.sample()
    new_space, reward, is_done, info = env.step(action)
    env.render()
env.close()                               # Close the window
```

Here is my simulation of running the game with random actions(Notice how the agent moves erratically).

<video width="320" height="240" controls margin-left: auto margin-right: auto display: block>
  <source src="/images/rl/pacman.mp4" type="video/mp4">
Random Action Gameplay
</video>
<!-- <a href="{/images/rl/pacman.mp4}" title="Random Action Gameplay"><img src="{/images/rl/pacman.png}" alt="PacMan random move agent" /></a> -->

<a name="myfootnote1">[1]</a> While I won't be explicitly going through PyTorch for brevity, I will explain the functions I use as they come up. Some level of experience with matrices and NumPy is inevitable.

