---
layout: post
title:  Dynamic Programming
subtitle: RL Basics
permalink: /blog/rl_dp/
tags: [Reinforcement Learning, Dynamic Programming, Markov Decision Process, RL Basics]
comments: true
share-img: https://raw.githubusercontent.com/shirsho-12/shirsho-12.github.io/master/images/rl/rl_theme.png
---

Dynamic Programming(DP) in reinforcement learning refers to a set of algorithms that can be used to compute optimal policies when the agent knows everything about its surroundings, i.e. a perfect model of the environment. Although dynamic programming has a large number of drawbacks, it is still the precursor to almost every other reinforcement learning algorithm. This makes knowing DP a foundational block in reinforcement learning. With that said, let's begin.

### Dynamic Programming - A Brief Introduction

*For those of you who know what dynamic programming is, feel free to skip this subsection.*

Dynamic programming, for the uninitiated, is reusing previously calculated values to calculate future ones. Suppose we had to calculate <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2"> and <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2 \times 2">. By calculating <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2 = 8"> first, we can be reuse it - <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2 \ times 2 = 8 \times 2 = 16"> - to reduce the number of total multiplications. This makes problems that would grow exponentially with size to become polynomial, i.e. much faster. In real terms, problems that may take over an hour to solve naively can be done in seconds. 

We perform dynamic programming by storing previously calculated values in a table. There are two main techniques of dynamic programming: tabulation and memoization. Both methods have their pros and cons. [GeeksForGeeks](https://www.geeksforgeeks.org/) has a whole series on [dynamic programming](https://www.geeksforgeeks.org/dynamic-programming/), which go all facets of dynamic programming.


## The Theoretical Jargon 
To my utter dismay, we can't proceed with this article without theory. Dynamic programming in RL has strict conditions we need to adhere to, which includes a lot of theory. 

### Markov Decision Process (MDP)

In the very first line of the article I've said that dynamic programming requires a *perfect model of the environment*. But what does this perfect model really mean? 

In our case, a perfect model means the agent has access to the likelihood of every action happening. The actions themselves must remain the same. Moreover, the probability of the agent moving from state `s` to state `s'` is conditionally independent - if the action takes place at time `t` it only depends on the probabilities at time `t-1`. The process is "memoryless". We call this type of probability matrix a **Markov Chain**. 

The importance of conditional independence may not be inherently obvious. An example of such independence would be in population growth. One only needs to know the current population and rate of natural increase(birth rate - death rate) to estimate the future population. Old data is not required. On the other hand, if you go to the doctor with a cough and chest pain, the doctor won't just look at your current state and give you a prescription. He or she will have to look at your medical history. Prior data is a must. 

In terms of reinforcement learning, conditional independence simplifies a lot of the mathematics involved. If the agent had to take all his previous actions into consideration each time it took a new one, training would take forever. And while many processes may not satisfy this Markov property, working under that assumption oftentimes leads to accurate results.

A reinforcement learning problem that satisfies this Markov property is called a **Markov Decision Process**. A *finite* task is one that has a clear beginning and ending, i.e. there is a known number of possible states and actions. If the task is finite, the problem becomes a *finite MDP*. As to why these things are important, the agent interacts with the environment and receives feedback in the form of a reward. At each time step t, the environment changes due to the agent's action. These changes are represented by the changing values in the Markov chain. 

<figure>
    <img src="/images/rl/agent-environment-interaction.png" desc="The Agent Environment Interaction Diagram">
    <figcaption>The Agent Environment Interaction Diagram Source: Sutton & Barlo</figcaption>
</figure>

### The Value Function
In reinforcement learning problem, our objective boils down to estimating the value of states (or of state-action pairs<sup>[1](#myfootnote1)</sup>). The agent's mapping of individual states to numbers is known as the **value function**. This value indicates "how good" a state is in terms of expected future reward. 

In practical terms a value function is a list of numbers; each number corresponding to the preference of a particular state. The process of refining the value function to reach the optimal policy is what dynamic programming is all about.

### Discounting - Not all rewards are created equal
By definition, the value of a state is its long-term reward potential. At time `t`, we need to consider the rewards of the states from time `t+1` to the end of the episode. But now a new question arises. *How should we value a reward in the future?* Depending on the task, we may choose to prioritize short-term rewards over long-term ones. The way we do this is by a process called **discounting**. The _expected<sup>[2](#myfootnote1)</sup> discounted reward_ at time `t`, <img src="https://render.githubusercontent.com/render/math?math=G_t"> is

<img src="https://render.githubusercontent.com/render/math?math=G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty}\gamma^k R_{t+k+1}" class="center">

<img src="https://render.githubusercontent.com/render/math?math=\gamma"> (<img src="https://render.githubusercontent.com/render/math?math=0 \leq \gamma \leq 1">) is known as the **discount rate**. 

The discount rate ensures that further the reward, the less is it is valued. This is particularly important if each episode is very long or infinite (the continuing task case). If we set <img src="https://render.githubusercontent.com/render/math?math=\gamma = 0">, the agent becomes _myopic_, only seeking to maximize the immediate reward at all times. In tasks such as pole balancing (the CartPole environment), a myopic strategy works perfectly well. And as <img src="https://render.githubusercontent.com/render/math?math=\gamma"> approaches 1, the agent becomes more _far-sighted_.

### Bellman Equations
Bellman optimality equations, or Bellman equations, are a set of equations defining the relationship between the value of a state and the value of its successor states. Solving this system of equations for a reinforcement learning task gives us the optimal value function. 

For the optimal policy <img src="https://render.githubusercontent.com/render/math?math=\pi_*"> and a state `s`, the optimal value function <img src="https://render.githubusercontent.com/render/math?math=v_{*}"> under the Bellman optimality equation is 

<figure>
    <img src="/images/rl/bellman_value_eq.png" desc="The Bellman Optimality Eqn for the state-value function" class="center">
</figure>

Yes, the equation looks terrifying, but what it says is relatively straightforward: the value of a state
under an optimal policy must equal the expected return for the best action from that state.  At a state `s`, we choose the action `a` that gives the maximum reward <img src="https://render.githubusercontent.com/render/math?math=R_{t+1}"> plus the discounted reward from the next state <img src="https://render.githubusercontent.com/render/math?math=\gamma v_{*}(S_{t+1})">.

The Bellman optimality equation for the optimal state <img src="https://render.githubusercontent.com/render/math?math=s_{*}"> is similar, only here we know the best action. 

<figure>
    <img src="/images/rl/bellman_state_eq.png" desc="The Bellman Optimality Eqn for the state function">
</figure>

Don't worry too much if you don't completely understand this subsection. The implementation is simpler than the equation.

## The FrozenLake problem



### Footnotes
<a name="myfootnote1">[1]</a> A state-action pair is the grouping of a state and a particular action into a single value. In a backup diagram, this would be the difference between assigning values to states themselves or to actions (state transitions).

<a name="myfootnote1">[2]</a> The term _expected return_ is used because we are calculating future rewards, which by nature is uncertain. Therefore, the expected return is a more sensible measure compared to trying to calculating the maximum reward from each state.  