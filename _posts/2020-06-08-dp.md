---
layout: post
title:  Dynamic Programming RL
subtitle: RL Basics
permalink: /blog/rl_dp/
tags: [Reinforcement Learning, Dynamic Programming, Markov Decision Process, RL Basics]
comments: true
share-img: https://raw.githubusercontent.com/shirsho-12/shirsho-12.github.io/master/images/rl/rl_theme.png
---

Dynamic Programming(DP) in reinforcement learning refers to a set of algorithms that can be used to compute optimal policies when the agent knows everything about its surroundings, i.e. a perfect model of the environment. Although dynamic programming has a large number of drawbacks, it is still the precursor to almost every other reinforcement learning algorithm. This makes knowing DP a foundational block in reinforcement learning. With that said, let's begin.

### Dynamic Programming - A Brief Introduction

*For those of you who know what dynamic programming is, feel free to skip this subsection.*

Dynamic programming, for the uninitiated, is reusing previously calculated values to calculate future ones. Suppose we had to calculate <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2"> and <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2 \times 2">. By calculating <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2 = 8"> first, we can be reuse it - <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2 \ times 2 = 8 \times 2 = 16"> - to reduce the number of total multiplications. This makes problems that would grow exponentially with size to become polynomial, i.e. much faster. In real terms, problems that may take over an hour to solve naively can be done in seconds. 

We perform dynamic programming by storing previously calculated values in a table. There are two main techniques of dynamic programming: tabulation and memoization. Both methods have their pros and cons. [GeeksForGeeks](https://www.geeksforgeeks.org/) has a whole series on [dynamic programming](https://www.geeksforgeeks.org/dynamic-programming/), which go all facets of dynamic programming.


## The Theoretical Jargon 
To my utter dismay, we can't proceed with this article without theory. Dynamic programming in RL has strict conditions we need to adhere to, which includes a lot of theory. 

### Markov Decision Process (MDP)

In the very first line of the article I've said that dynamic programming requires a *perfect model of the environment*. But what does this perfect model really mean? 

In our case, a perfect model means the agent has access to the likelihood of every action happening. The actions themselves must remain the same. Moreover, the probability of the agent moving from state `s` to state `s'` is conditionally independent - if the action takes place at time `t` it only depends on the probabilities at time `t-1`. The process is "memoryless". We call this type of probability matrix a **Markov Chain**. 

The importance of conditional independence may not be inherently obvious. An example of such independence would be in population growth. One only needs to know the current population and rate of natural increase(birth rate - death rate) to estimate the future population. Old data is not required. On the other hand, if you go to the doctor with a cough and chest pain, the doctor won't just look at your current state and give you a prescription. He or she will have to look at your medical history. Prior data is a must. 

In terms of reinforcement learning, conditional independence simplifies a lot of the mathematics involved. If the agent had to take all his previous actions into consideration each time it took a new one, training would take forever. And while many processes may not satisfy this Markov property, working under that assumption oftentimes leads to accurate results.

A reinforcement learning problem that satisfies this Markov property is called a **Markov Decision Process**. A *finite* task is one that has a clear beginning and ending, i.e. there is a known number of possible states and actions. If the task is finite, the problem becomes a *finite MDP*. As to why these things are important, the agent interacts with the environment and receives feedback in the form of a reward. At each time step t, the environment changes due to the agent's action. These changes are represented by the changing values in the Markov chain. 

<figure>
    <img src="/images/rl/agent-environment-interaction.png" desc="The Agent Environment Interaction Diagram">
    <figcaption>XKCD #1955</figcaption>
</figure

### Bellman Equations
Bellman optimality equations, or Bellman equations, are a set of equations defining the relationship between the value of a state and the value of its successor states. 

