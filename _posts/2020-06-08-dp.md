---
layout: post
title:  Dynamic Programming
subtitle: RL Basics
permalink: /blog/rl_dp/
tags: [Reinforcement Learning, Dynamic Programming, Markov Decision Process, RL Basics]
comments: true
share-img: https://raw.githubusercontent.com/shirsho-12/shirsho-12.github.io/master/images/rl/rl_theme.png
---

Dynamic Programming(DP) in reinforcement learning refers to a set of algorithms that can be used to compute optimal policies when the agent knows everything about its surroundings; i.e. the agent has a perfect model of the environment. Although dynamic programming has a large number of drawbacks, it is the precursor to almost every other reinforcement learning algorithm. This makes knowing DP a necessity in understanding reinforcement learning. With that said, let's begin.

### Dynamic Programming - A Brief Introduction

*For those of you who know what dynamic programming is, feel free to skip this subsection.*

Dynamic programming, for the uninitiated, is reusing previously calculated values to compute the next value. Suppose we have to find the answer of <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2"> and <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2 \times 2">. By calculating <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2 = 8"> first, we can be reuse it - <img src="https://render.githubusercontent.com/render/math?math=2 \times 2 \times 2 \times 2 = 8 \times 2 = 16"> - to reduce the total number of multiplications. This makes problems that would otherwise grow exponentially with size to become polynomial - a major speed-up. In real terms, problems that may take over an hour to solve naively can be done in seconds. 

We perform dynamic programming by storing previously calculated values in a table. There are two main techniques of dynamic programming: tabulation and memoization. Both methods have their pros and cons. [GeeksForGeeks](https://www.geeksforgeeks.org/) has a whole series on [dynamic programming](https://www.geeksforgeeks.org/dynamic-programming/), which goes in-depth into the topic.


## The Theoretical Jargon 
To my utter dismay, we can't proceed with this article without theory. Dynamic programming in RL has strict conditions we need to adhere to, which includes a lot of theory. 

### Markov Decision Process (MDP)

In the very first line of the article I've said that dynamic programming requires a *perfect model of the environment*. But what does this perfect model really mean? 

In our case, a perfect model means the agent has access to the likelihood of every action happening. The actions themselves must remain the same. Moreover, the probability of the agent moving from state `s` to state `s'` is conditionally independent - if the action takes place at time `t` it only depends on the probabilities at time `t-1`. The process is "memoryless". The representation of the action space is a type of probability matrix a known as a **Markov Chain**. 

The importance of conditional independence may not be inherently obvious. An example of such independence would be in population growth. One only needs to know the current population and rate of natural increase(birth rate - death rate) to estimate the future population. Old data is not required. On the other hand, if you go to the doctor with a cough and chest pain, the doctor won't just look at your current state and give you a prescription. He or she will have to look at your medical history. Prior data is a must. 

In terms of reinforcement learning, conditional independence simplifies much of the mathematics involved. If the agent had to take all his previous actions into consideration each time it took a new one, training would take forever. And while many processes may not satisfy this Markov property, working under that assumption usually leads to accurate results.

A reinforcement learning problem that satisfies this Markov property is called a **Markov Decision Process**. A *finite* task is one that has a clear beginning and ending, i.e. there is a known number of possible states and actions. If the task is finite, the problem becomes a *finite MDP*. The agent interacts with the environment and receives feedback in the form of a reward. At each time step t, the environment changes due to the agent's action. These changes are represented by the changing values in the Markov chain. 

<figure>
    <img src="/images/rl/agent-environment-interaction.png" desc="The Agent Environment Interaction Diagram">
    <figcaption>The Agent Environment Interaction Diagram. Source: Sutton & Barlo</figcaption>
</figure>

### The Value Function
In reinforcement learning problem, our objective boils down to estimating the value of states (or of state-action pairs<sup>[1](#myfootnote1)</sup>). The agent's mapping of individual states to numbers is known as the **value function**. This value indicates "how good" a state is in terms of expected future reward. 

In practical terms a value function is a list of numbers: each number corresponding to the preference of a particular state. The process of refining the value function to reach the optimal policy is what dynamic programming is all about.

### Discounting - Not all rewards are created equal
By definition, the value of a state is its long-term reward potential. At time `t`, we need to consider the rewards of the states from time `t+1` to the end of the episode. But now a new question arises. *How should we differentiate between an instanteneous reward with one that is further into the future?* Depending on the task, we may choose to prioritize short-term rewards over long-term ones. The way we do this is by a process called **discounting**. The _expected<sup>[2](#myfootnote1)</sup> discounted reward_ at time `t`, <img src="https://render.githubusercontent.com/render/math?math=G_t"> is

<img src="https://render.githubusercontent.com/render/math?math=G_t = R_{t %2B 1} %2B \gamma R_{t %2B 2} %2B \gamma^2 R_{t %2B 3} %2B \cdots = \sum_{k=0}^{\infty}\gamma^k R_{t %2B k %2B 1}" class="center">

<img src="https://render.githubusercontent.com/render/math?math=\gamma"> (<img src="https://render.githubusercontent.com/render/math?math=0 \leq \gamma \leq 1">) is known as the **discount rate**. 

The discount rate ensures that further the reward, the less is it is valued. This is particularly important if each episode is very long or infinite (the continuing task case). If we set <img src="https://render.githubusercontent.com/render/math?math=\gamma = 0">, the agent becomes _myopic_, only seeking to maximize the immediate reward at all times. In tasks such as pole balancing (the CartPole environment), a myopic strategy works perfectly well. And as <img src="https://render.githubusercontent.com/render/math?math=\gamma"> approaches 1, the agent becomes more _far-sighted_. As long as the discount rate is less than 1, the infinite sum is bound to converge.

### Bellman Equations
Bellman optimality equations, or Bellman equations, are a set of equations defining the relationship between the value of a state and the value of its successor states. Solving this system of equations for a reinforcement learning task gives us the optimal value function. 

For the optimal policy <img src="https://render.githubusercontent.com/render/math?math=\pi_*"> and an arbitrary state `s`, the optimal value function <img src="https://render.githubusercontent.com/render/math?math=V_{*}"> under the Bellman optimality equation is 

<figure>
    <img src="/images/rl/bellman_value_eq.png" desc="The Bellman Optimality Eqn for the state-value function"> 
    <!-- decentralized -->
</figure>

Yes, the equation looks terrifying, but what it says is relatively straightforward: the value of a state
under an optimal policy must equal the expected return for the best action from that state.  At a state `s`, we choose the action `a` that gives the maximum reward <img src="https://render.githubusercontent.com/render/math?math=R_{t %2B 1}"> plus the discounted reward from the next state <img src="https://render.githubusercontent.com/render/math?math=\gamma V_{*}(S_{t %2B 1})">. 

The Bellman optimality equation for the optimal state <img src="https://render.githubusercontent.com/render/math?math=s_{*}"> is similar, only here we know the best action. 

<figure>
    <img src="/images/rl/bellman_state_eq.png" desc="The Bellman Optimality Eqn for the state function">
</figure>

Finding the inverse of the Markov chain will result in the solution of the Bellman equation for the value function. However, inverting a matrix is a slow, computationally expensive task. Dynamic programming algorithms attempt to find a good approximation to this inverse. 

Don't worry too much if you don't completely understand this subsection. The implementation is simpler than the equation.

## Walking on a FrozenLake

We will now go through and implement the dynamic programming algorithms on OpenAI Gym's [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/) environment.

### The FrozenLake
OpenAI's FrozenLake environment is a 4 by 4 or 8 by 8 grid representing an icy lake. The goal here is to go from the starting position S to the finish F while avoiding the holes H. The lore is as follows:

> Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. (Source: OpenAI)


```py
env = gym.make("FrozenLake-v0") 
env.reset()
env.render()
>>> SFHF            # (S: start)
    FHFH            # (F: frozen, safe)
    FFFH            # (H: hole, failure)
    HHFG            # (G: goal)
```

There are 16 possible positions the agent can be on, i.e. 16 possible states. The agent can take 4 actions: LEFT(0), DOWN(1), UP(2) and RIGHT(3).

```py
num_states = env.observation_space.n
num_actions = env.action_space.n
print(num_states, ", ", num_actions)
>>> 16, 4
```

The special factor about this environment is that the agent often does not do what we tell it to. I demonstrate this below

```py
state = env.reset()
print(state)
>>> 0        # Starting position
# Let's make the agent go a step downwards
new_state, reward, is_done, info = env.step(1)  
print(new_state)  
>>> 1       # The agent went to the left instead
print(reward)
>>> 0     
```
The agent went the other way instead. Let's print the `info` variable to see the reason.

```py
print(info)
>>> {'prob': 0.3333333333333333}
```
The probability of the agent performing a particular action is 1/3. Not the easiest environment to traverse. The reward is 1 at the goal, and 0 everywhere else. This makes the environment impossible for the random search algorithm to solve. Feel free to try it out on your own. I personally found my agent successfully traversing FrozenLake less than 15% of the time (given 1000 attempts).

### Iteration: Evaluation and Improvement

Iteration, evaluation, and improvement were things I often mixed up when I first started learning this topic. We evaluate policies. By this I mean that for a new policy, we update our value function based on the new action taken and the previous set of values. This update process follows the _Bellman Optimality Equation for the value function_.

After we have our updated values, we use this to *improve* our policy. This is done by choosing the actions that have the highest updated values (greedy). The equation used here is the _Bellman Optimality Equation for the optimal state_.

Now that we have a new policy, we evaluate this policy again. This cyclic process is known as **iteration**. Starting with arbitrary value and policy functions, the iteration process leads us to the optimal <img src="https://render.githubusercontent.com/render/math?math=V_{*}"> and <img src="https://render.githubusercontent.com/render/math?math=\pi_{*}"> sooner or later.  

Policy improvement cycle   |  The goal of policy improvement
:-------------------------:|:-------------------------:
![Policy Improvement Cycle](/images/rl/iteration.png)  |  ![Policy Improvement Goal](/images/rl/iteration_goal.png)


### Value Iteration

#### Footnotes
<a name="myfootnote1">[1]</a> A state-action pair is the grouping of a state and a particular action into a single value. In a backup diagram, this would be the difference between assigning values to states themselves or to actions (state transitions).

<a name="myfootnote1">[2]</a> The term _expected return_ is used because we are calculating future rewards, which by nature is uncertain. Therefore, the expected return is a more sensible measure compared to trying to calculating the maximum reward from each state.  