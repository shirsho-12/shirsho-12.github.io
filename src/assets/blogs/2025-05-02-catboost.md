---
layout: post
title: CatBoost: Gradient Boosting with Categorical Features Support
subtitle: Machine Learning
permalink: /blog/catboost/
tags: [Gradient Boosting, Machine Learning, Categorical Features]
featured: true
comments: true
---

CatBoost <sup>[1](#paper)</sup> is a gradient boosting algorithm that supports categorical features and uses them during training instead of preprocessing them into numerical values. The [library](https://github.com/catboost/catboost) has both CPU and GPU support, making it efficient for large datasets (training on GPU and metrics on CPU).

## Boosting

Boosting is an ensemble learning technique that combines the predictions of multiple weak learners to create a strong learner. The idea is to train a sequence of models, where each model tries to correct the errors made by the previous ones. The final prediction is obtained by combining the predictions of all models, typically through weighted averaging.

Generally, the weak learners are decision trees, and the boosting process involves the following steps:

1. **Initialization**: Start with an initial prediction, often the mean of the target variable.
2. **Iterative Training**: For each iteration:
   - Compute the residuals (errors) of the current model.
   - Train a new weak learner (e.g., a decision tree) on the residuals.
   - Update the predictions by adding the predictions of the new weak learner, scaled by a learning rate.
3. **Final Prediction**: The final prediction is the sum of the initial prediction and the predictions from all weak learners.
4. **Regularization**: Apply techniques like shrinkage (learning rate) and subsampling to prevent overfitting.
5. **Stopping Criteria**: Use validation data to monitor performance and stop training when performance starts to degrade.

## CatBoost

Given a dataset of observations $\mathcal{D} = \{(X_i, Y_i)\}_{i=1}^N$, where $X_i = (x_{i1}, x_{i2}, \cdots, x_{im})$ is a vector of $m$ features and $Y_i$ is the label value. A straightforward way to handle categorical features is with the average label value on the training set.

$$
x_{ik}=\frac{
    \sum_{j=1}^{p-1} \left[ x_{\sigma_j, k} = x_{\sigma_p, k} \right] Y_{\sigma_j}
}{
    \sum_{j=1}^{p-1} \left[ x_{\sigma_j, k} = x_{\sigma_p, k} \right]
}
$$

However, this approach leads to overffitting. For example, if we have a categorical feature value that appears only once in the training set, the average label value will be equal to the label value of that observation. To avoid this CatBoost uses a technique called **ordered boosting**. The idea is to use first perform a random permutation of the dataset and then use the average label value of the previous observations to compute the current average label value.

$$
\frac{
    \sum_{j=1}^{p-1} \left[ x_{\sigma_j, k} = x_{\sigma_p, k} \right] Y_{\sigma_j} + a \cdot P
}{
    \sum_{j=1}^{p-1} \left[ x_{\sigma_j, k} = x_{\sigma_p, k} \right] + a
}
$$

Where $P$ is the average label value of the previous observations, i.e., the prior, and $a>0$ is a hyperparameter which is the weight of the prior.

### Unbiased Gradients

Classical boosting algorithms suffer from overfitting caused by the problem of biased pointwise gradient estimates. Gradients are estimated at each step using the same data points that are used to train the model. This leads to a shift in the distribution of estimated gradients with respect to the true distribution of gradients.

To make the gradient $g^i(X_k, Y_k)$ unbiased w.r.t. to the weak learner $F^i$, the model $F^i$ must be trained without the observation $X_k$. To simulate this, a separate model $M_k$ is trained on the dataset $\mathcal{D} \setminus \{X_k\}$, and the gradient is computed as follows:

$
M_i \leftarrow 0 \text{ for }  i = 1 \ldots n \\
\textbf{for } \textit{iter} \leftarrow 1 \text{ to } I \textbf{ do}\\
\quad \textbf{for} i \leftarrow 1 \text{ to } n \textbf{ do}\\
\quad\quad \textbf{for } j \leftarrow 1 \text{ to } i-1 \textbf{ do}\\
\quad\quad\quad g_j \leftarrow \left.\frac{\partial}{\partial a} \text{ Loss}(y_j, a) \right|_{a=M_i(X_j)} \\
\quad\quad M \leftarrow \text{LearnOneTree}(\{(X_j, g_j)\}_{j=1}^{i-1}) \\
\quad\quad M_i \leftarrow M_i + M \\
\textbf{Return: } M_1, \ldots, M_n; M_1(X_1), M_2(X_2), \ldots, M_n(X_n) \\
$

### Fast Scorer

CatBoost uses oblivious trees as base predictors. An oblivious tree is a decision tree where all nodes at the same depth use the same splitting feature and the same splitting condition. Each leaf index can be encoded as a binary vector, where each bit represents whether the corresponding feature is used in the split at that level. This encoding allows for fast computation of the tree's predictions, as the same feature is used at each level of the tree. These trees can be built in parallel, which gives up to a $3\times$ speedup.

### Table: Scorer comparison

|              | 1 thread     | 32 threads   |
| ------------ | ------------ | ------------ |
| **CatBoost** | 2.4s         | 231ms        |
| **XGBoost**  | 78s (×32.5)  | 4.5s (×19.5) |
| **LightGBM** | 122s (×50.8) | 17.1s (×74)  |

### Conclusion

In this post, we discussed the CatBoost algorithm, which is a gradient boosting algorithm that supports categorical features and uses them during training instead of preprocessing them into numerical values. We also discussed the boosting process, the problem of biased pointwise gradient estimates, and how CatBoost uses ordered boosting to avoid overfitting. Finally, we discussed the fast scorer used in CatBoost, which allows for fast computation of the tree's predictions.

### Footnotes

<a name="paper">[1]</a> Dorogush, Anna Veronika, Vasily Ershov, and Andrey Gulin. "CatBoost: Gradient Boosting with Categorical Features Support." arXiv preprint arXiv:1810.11363 (2018).

$$
$$
